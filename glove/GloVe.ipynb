{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GloVe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAQgM7pY3u3V"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMxONIZcrVZE",
        "outputId": "e5d97baa-89fa-4f8e-8094-b4e5f7df5b60"
      },
      "source": [
        "### Mount to drive ###\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import tqdm\n",
        "import string\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### Tensorflow dependencies ###\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence # For custom data generator\n",
        "\n",
        "### nltk dependencies ###\n",
        "!python3 -m nltk.downloader stopwords\n",
        "!python3 -m nltk.downloader wordnet \n",
        "!python3 -m nltk.downloader punkt\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "### Some constants ###\n",
        "true_path = '/content/drive/My Drive/Fake.csv'\n",
        "fake_path  = '/content/drive/My Drive/True.csv'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3q9rF8M4i01"
      },
      "source": [
        "# I. Loading and cleaning data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIhV-Cbp4twj"
      },
      "source": [
        "# Sample data with each class taking 10000 instances\n",
        "df_fake = pd.read_csv(fake_path, header=0).sample(n=10000, random_state=np.random.randint(0,40000))\n",
        "df_true = pd.read_csv(true_path, header=0).sample(n=10000, random_state=np.random.randint(0,40000))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "XaQFNwqN4XUs",
        "outputId": "8b84cdb0-7443-4b97-fdeb-2d97fffc9d33"
      },
      "source": [
        "# Concatenate the two data frames \n",
        "df_fake['label'] = 0\n",
        "df_true['label'] = 1\n",
        "\n",
        "df = pd.concat([df_fake, df_true])\n",
        "df.index = list(range(len(df.index))) # re-index the dataframe\n",
        "df # print out the data frame"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Problems pile up for unlucky village near epic...</td>\n",
              "      <td>HUAUTLA, Mexico (Reuters) - Yesenia Vergara gr...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>September 24, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Immigrants in Central Florida nervous about se...</td>\n",
              "      <td>APOPKA, Fla. (Reuters) - With Hurricane Irma b...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>September 9, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>U.S. appeals court will not put Trump travel b...</td>\n",
              "      <td>SAN FRANCISCO (Reuters) - A federal appeals co...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>February 27, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Clinton will hold election night rally in New ...</td>\n",
              "      <td>WASHINGTON (Reuters) - U.S. Democratic preside...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>October 26, 2016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Senate Republican leader starts clock ticking ...</td>\n",
              "      <td>WASHINGTON (Reuters) - Senate Republican leade...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>April 4, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>GERMANY’S DEFENSE MINISTER Refuses To Wear Hij...</td>\n",
              "      <td>Germany s defense minister refused to wear a t...</td>\n",
              "      <td>politics</td>\n",
              "      <td>Dec 14, 2016</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>HERE WE GO: GEORGIA POLITICIAN Calls for Remov...</td>\n",
              "      <td>The politicians are using our monuments to his...</td>\n",
              "      <td>politics</td>\n",
              "      <td>Aug 15, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>Conservative Conspiracy Theorist ARRESTED Aft...</td>\n",
              "      <td>Lucy Richards, a Tampa woman who believes luna...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 8, 2016</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>‘Might Have Been Faked By Liberals’: Top Advi...</td>\n",
              "      <td>I mean, that headline, right? Did someone actu...</td>\n",
              "      <td>News</td>\n",
              "      <td>August 8, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>WATCH: Michelle Obama DESTROYS Donald Trump W...</td>\n",
              "      <td>First Lady Michelle Obama brought the house do...</td>\n",
              "      <td>News</td>\n",
              "      <td>July 26, 2016</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ... label\n",
              "0      Problems pile up for unlucky village near epic...  ...     0\n",
              "1      Immigrants in Central Florida nervous about se...  ...     0\n",
              "2      U.S. appeals court will not put Trump travel b...  ...     0\n",
              "3      Clinton will hold election night rally in New ...  ...     0\n",
              "4      Senate Republican leader starts clock ticking ...  ...     0\n",
              "...                                                  ...  ...   ...\n",
              "19995  GERMANY’S DEFENSE MINISTER Refuses To Wear Hij...  ...     1\n",
              "19996  HERE WE GO: GEORGIA POLITICIAN Calls for Remov...  ...     1\n",
              "19997   Conservative Conspiracy Theorist ARRESTED Aft...  ...     1\n",
              "19998   ‘Might Have Been Faked By Liberals’: Top Advi...  ...     1\n",
              "19999   WATCH: Michelle Obama DESTROYS Donald Trump W...  ...     1\n",
              "\n",
              "[20000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l82O73U35dM6"
      },
      "source": [
        "## 1. Removing tags and accented chars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1EHC8UO5RXy",
        "outputId": "437ba1f6-94ec-4c62-85ed-82e14617fcb9"
      },
      "source": [
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "print('[INFO] Clearing html tags ...')\n",
        "df['title'] = df['title'].apply(lambda x : cleanhtml(x))\n",
        "df['text'] = df['text'].apply(lambda x : cleanhtml(x))\n",
        "print('Done!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Clearing html tags ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-TAbhAa6Z_K",
        "outputId": "285c6ed6-d081-438c-f4c7-083018d36991"
      },
      "source": [
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii.decode()\n",
        "\n",
        "print(\"[INFO] Removing accented characters ... \")\n",
        "df['title'] = df['title'].apply(lambda x : remove_accents(x))\n",
        "df['text'] = df['text'].apply(lambda x : remove_accents(x))\n",
        "print(\"Done!\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Removing accented characters ... \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYDtpHR17x-Y"
      },
      "source": [
        "## 2. Removing contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyrHNQSI7zxg",
        "outputId": "58420ae5-2b7e-4672-d75c-1d8e9ff1afeb"
      },
      "source": [
        "# Dictionary of English Contractions\n",
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "# Regular expression for finding contractions\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "# Function for expanding contractions\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "print(\"[INFO] Expanding contraction ... \")\n",
        "df['title'] = df['title'].apply(lambda x : expand_contractions(x))\n",
        "df['text'] = df['text'].apply(lambda x : expand_contractions(x))\n",
        "print(\"[INFO] Done!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Expanding contraction ... \n",
            "[INFO] Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF_haJFw8d1l"
      },
      "source": [
        "## 3. Lowercasing text and punctuations removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CHBuubd78C0",
        "outputId": "6496e9ab-11e1-4d07-eb4f-d73426bd0db3"
      },
      "source": [
        "print('[INFO] Lowercasing text ... ')\n",
        "df['title'] = df['title'].apply(lambda x : x.lower())\n",
        "df['text'] = df['text'].apply(lambda x : x.lower())\n",
        "print('[INFO] Done!')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Lowercasing text ... \n",
            "[INFO] Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wFeNhnO8JWe",
        "outputId": "438b1ac8-dfe3-4f05-d909-65d588dc2e26"
      },
      "source": [
        "print('[INFO] Removing punctuations ... ')\n",
        "df['title']=df['title'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
        "df['text']=df['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
        "print('[INFO] Done!')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Removing punctuations ... \n",
            "[INFO] Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuiUNLuX83_T"
      },
      "source": [
        "## 4. Removing special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtprrPUA8mQY",
        "outputId": "83e3ff25-4837-4d31-d8c2-d5037005ae80"
      },
      "source": [
        "print('[INFO] Removing non-text characters ...')\n",
        "df['title'] = df['title'].apply(lambda x : re.sub('[^A-Za-z0-9]+', ' ', x))\n",
        "df['text'] = df['text'].apply(lambda x : re.sub('[^A-Za-z0-9]+', ' ', x))\n",
        "print('Done!')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Removing non-text characters ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v-OGCD88m9M",
        "outputId": "3bbeea57-f095-44bb-eb59-1915f23f7429"
      },
      "source": [
        "print('[INFO] Removing extra spaces ...')\n",
        "df['title']=df['title'].apply(lambda x: re.sub(' +',' ',x))\n",
        "df['text']=df['text'].apply(lambda x: re.sub(' +',' ',x))\n",
        "print('[INFO] Done!')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Removing extra spaces ...\n",
            "[INFO] Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWmsgdLy-ljg",
        "outputId": "b2a06e82-0ec0-4909-a4cc-4208d30e7b46"
      },
      "source": [
        "print('[INFO] Removing digits ... ')\n",
        "df['title']=df['title'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
        "df['text']=df['text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
        "print('[INFO] Done!')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Removing digits ... \n",
            "[INFO] Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehqJFrOe9zQ_"
      },
      "source": [
        "## 5. Stemming & lemmatization and stopwords removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e9yoV9Y9L1R",
        "outputId": "3cb00b32-edbd-4606-e790-0bc588dc054f"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) # Assuming all nltk data is installed\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def filter_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = ' '.join(filtered)\n",
        "\n",
        "    ### Removing all special characters ###\n",
        "    special_chars = ['…', '–', '’', '‘', '”', '“']\n",
        "\n",
        "    for char_ in special_chars:\n",
        "      filtered_sentence = filtered_sentence.replace(char_, \"\")\n",
        "\n",
        "    # Removal of extra spaces\n",
        "    filtered_sentence = re.sub(' +',' ',filtered_sentence)\n",
        "    return filtered_sentence\n",
        "\n",
        "print('[INFO] Removing stop words and lemmatizing ... ')\n",
        "df['title'] = df['title'].apply(lambda x : filter_stopwords(x))\n",
        "df['text'] = df['text'].apply(lambda x : filter_stopwords(x))\n",
        "print('[INFO] Done!')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Removing stop words and lemmatizing ... \n",
            "[INFO] Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufVOfX8-_ELK"
      },
      "source": [
        "# II. Text data vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQRE7eVR9qzb",
        "outputId": "a3a6a35c-c2cf-4fbd-a283-9368d31c43dd"
      },
      "source": [
        "features_text = df['text'].values\n",
        "labels = df['label'].values\n",
        "vfunc = np.vectorize(lambda x : len(x.split(' ')))\n",
        "all = ' '.join(features_text)\n",
        "vocab_size = len(list(set(all.split(\" \")))) + 1\n",
        "max_seq_len = vfunc(features_text).max()\n",
        "max_vocab = 10000\n",
        "\n",
        "tokenizer = Tokenizer(char_level=False, oov_token='<PAD>')\n",
        "tokenizer.fit_on_texts(features_text)\n",
        "features = tokenizer.texts_to_sequences(features_text)\n",
        "features = pad_sequences(features, maxlen=max_seq_len)\n",
        "\n",
        "print(np.max(np.array(features).flatten()))\n",
        "print(f'[INFO] Vocabulary size {vocab_size}')\n",
        "print(f'[INFO] Max sequence length : {max_seq_len}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "124427\n",
            "[INFO] Vocabulary size 124428\n",
            "[INFO] Max sequence length : 4561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8ud2uqJ9-wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654ee786-5245-4664-9d54-2c01d5f8c484"
      },
      "source": [
        "def get_co_occurrence_frequency(x1, x2, corpus):\n",
        "  if(x1 > 0 and x2 > 0): # if both words are non oov_token\n",
        "    contains_x1 = [x for x in corpus if x1 in x]\n",
        "    contains_x1_and_x2 = [x for x in contains_x1 if x2 in x]\n",
        "\n",
        "    return len(contains_x1_and_x2)\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def get_docs_with_term(x, corpus):\n",
        "  if(x > 0):\n",
        "    contains_x = [x_ for x_ in corpus if x in x_]\n",
        "    return np.array(contains_x)\n",
        "  else:\n",
        "    return np.array([])\n",
        "\n",
        "word2word_cooccurrence = {}\n",
        "with tqdm.tqdm(total=vocab_size) as pbar:\n",
        "  for x in range(1, vocab_size):\n",
        "    contains_x = [x_ for x_ in features if x in x_]\n",
        "    contains_x = np.array(contains_x)\n",
        "    co_occur_with_x = np.unique(contains_x.flatten())\n",
        "\n",
        "    word2word_cooccurrence[x] = co_occur_with_x\n",
        "    pbar.update(1)\n",
        "\n",
        "class WordPairGenerator(Sequence):\n",
        "  def __init__(self, corpus, vocab_size, batch_size):\n",
        "    self.corpus = corpus \n",
        "    self.vocab_size = vocab_size \n",
        "    self.batch_size = batch_size\n",
        "    self.vocabs = np.unique(corpus.flatten())\n",
        "\n",
        "  def __len__(self):\n",
        "    ''' Denotes number of batches per epoch '''\n",
        "    return self.corpus.shape[0] // self.batch_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    ''' Generate one batch of data '''\n",
        "    words1 = []\n",
        "    words2 = []\n",
        "    frequencies = []\n",
        "    num_items = 0\n",
        "\n",
        "    while(num_items < self.batch_size):\n",
        "      x1 = np.random.randint(1, self.vocab_size + 1)\n",
        "      docs_with_x1 = get_docs_with_term(x1, self.corpus)\n",
        "      x2 = np.random.choice(np.unique(docs_with_x1.flatten()), 1)[0]\n",
        "\n",
        "      frequency = get_co_occurrence_frequency(x1, x2, self.corpus)\n",
        "      if(frequency == 0):\n",
        "        continue\n",
        "\n",
        "      words1.append(x1)\n",
        "      words2.append(x2)\n",
        "      frequencies.append(float(frequency))\n",
        "      num_items += 1\n",
        "\n",
        "    return [np.array(words1), np.array(words2)], np.array(frequencies)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 601/124428 [03:32<7:59:24,  4.30it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APMvVK-4RZBg"
      },
      "source": [
        "# III. Model and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CB1Z_BBCKve"
      },
      "source": [
        "X_MAX = 1000\n",
        "a = 3.0 / 4.0\n",
        "\n",
        "def glove_model(vocab_size=10, vector_dim=3):\n",
        "    \"\"\"\n",
        "    A Keras implementation of the GloVe architecture\n",
        "    :param vocab_size: The number of distinct words\n",
        "    :param vector_dim: The vector dimension of each word\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    input_target = Input((1,), name='central_word_id')\n",
        "    input_context = Input((1,), name='context_word_id')\n",
        "\n",
        "    central_embedding = Embedding(vocab_size, vector_dim, input_length=1)\n",
        "    central_bias = Embedding(vocab_size, 1, input_length=1)\n",
        "\n",
        "    context_embedding = Embedding(vocab_size, vector_dim, input_length=1)\n",
        "    context_bias = Embedding(vocab_size, 1, input_length=1)\n",
        "\n",
        "    vector_target = central_embedding(input_target)\n",
        "    vector_context = context_embedding(input_context)\n",
        "\n",
        "    bias_target = central_bias(input_target)\n",
        "    bias_context = context_bias(input_context)\n",
        "\n",
        "    dot_product = Dot(axes=-1)([vector_target, vector_context])\n",
        "    dot_product = Reshape((1, ))(dot_product)\n",
        "    bias_target = Reshape((1,))(bias_target)\n",
        "    bias_context = Reshape((1,))(bias_context)\n",
        "\n",
        "    prediction = Add()([dot_product, bias_target, bias_context])\n",
        "\n",
        "    model = Model(inputs=[input_target, input_context], outputs=prediction)\n",
        "    model.compile(loss=custom_loss, optimizer=Adam())\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    This is GloVe's loss function\n",
        "    :param y_true: The actual values, in our case the 'observed' X_ij co-occurrence values\n",
        "    :param y_pred: The predicted (log-)co-occurrences from the model\n",
        "    :return: The loss associated with this batch\n",
        "    \"\"\"\n",
        "    return K.sum(K.pow(K.clip(y_true / X_MAX, 0.0, 1.0), a) * K.square(y_pred - K.log(y_true)), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "hVqX6VwdRjjB",
        "outputId": "9a6fc37b-f483-48f7-88b3-5b7e0aed2400"
      },
      "source": [
        "model = glove_model(vocab_size=vocab_size, vector_dim=3)\n",
        "train_generator = WordPairGenerator(features, vocab_size, 32)\n",
        "model.fit(train_generator, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 78/625 [==>...........................] - ETA: 1:05:30 - loss: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-2e2049159ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordPairGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym_6JdUERqRu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}