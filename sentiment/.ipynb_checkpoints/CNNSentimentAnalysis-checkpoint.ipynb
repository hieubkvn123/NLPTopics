{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Tensorflow dependencies ###\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "### NLTK dependencies ### \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "### Others ###\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Some constants ###\n",
    "BASE_DIR = \"./\" # \"/content/drive/My Drive/\"\n",
    "WEIGHTS_DIR = os.path.join(BASE_DIR, \"weights\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "BASIC_CNN_MODEL_NAME = \"basic_cnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Took 48.31170916557312 seconds to preprocess data from ../data/sentiment_train.tsv\n",
      "[INFO] Took 20.135953903198242 seconds to preprocess data from ../data/sentiment_test.tsv\n"
     ]
    }
   ],
   "source": [
    "### 1. Loading data from files and preprocessing ###\n",
    "def en_contraction_expand(sentence):\n",
    "    # Dictionary of English Contractions\n",
    "    contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "    contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    \n",
    "    return contractions_re.sub(replace, sentence)\n",
    "\n",
    "def filter_stopwords(text):\n",
    "    ### Stopword removal using the nltk way ###\n",
    "    stop_words = set(stopwords.words('english')) # Assuming all nltk data is installed\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    filtered_sentence = ' '.join(filtered)\n",
    "    return filtered_sentence\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 1. expand contraction for english\n",
    "    sentence = en_contraction_expand(sentence)\n",
    "    \n",
    "    # 2. lowercasing text\n",
    "    sentence = sentence.strip().lower()\n",
    "    \n",
    "    # 3. Digits removal\n",
    "    sentence = re.sub('\\w*\\d\\w*','', sentence)\n",
    "\n",
    "    # 4. Punctuations removal\n",
    "    sentence = re.sub('[%s]' % re.escape(string.punctuation), '', sentence)\n",
    "    \n",
    "    # 5. Extra spaces removal\n",
    "    sentence = re.sub(' +',' ', sentence)\n",
    "    \n",
    "    # 5. filter stopwords \n",
    "    sentence = filter_stopwords(sentence)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def load_data_from_tsv(data_file, test=False):\n",
    "    start = time.time()\n",
    "    data = pd.read_table(data_file, header=0)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    dropped_cols = [\"PhraseId\", \"SentenceId\"]\n",
    "    data.drop(dropped_cols, inplace=True, axis=1)\n",
    "    \n",
    "    sentences = data['Phrase'].values\n",
    "    \n",
    "    if(test):\n",
    "        sentiments = None\n",
    "    else:\n",
    "        sentiments = data['Sentiment'].values\n",
    "    \n",
    "    sentences = [preprocess_sentence(x) for x in sentences]\n",
    "    sentences = np.array(sentences)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'[INFO] Took {end - start} seconds to preprocess data from {data_file}')\n",
    "    \n",
    "    return sentences, sentiments\n",
    "\n",
    "X_train, Y_train = load_data_from_tsv(\"../data/sentiment_train.tsv\")\n",
    "X_test, Y_test  = load_data_from_tsv(\"../data/sentiment_test.tsv\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of sentences in training set : 156060\n",
      "[INFO] Number of sentences in testing set  : 66292\n",
      "[INFO] Maximum sentence length in          : 30\n",
      "[INFO] Vocabulary size                     : 17280\n"
     ]
    }
   ],
   "source": [
    "### 2. Vectorizing data for sentiment analysis ###\n",
    "tokenizer = Tokenizer(char_level=False, oov_token=\"PAD\") # 0 is a reserved index\n",
    "tokenizer.fit_on_texts(np.concatenate([X_train, X_test]))\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq  = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "### Sequence padding ###\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "\n",
    "train_seq = pad(train_seq)\n",
    "test_seq  = pad(test_seq, length=train_seq.shape[-1])\n",
    "\n",
    "vocab_size = np.max(np.concatenate([train_seq, test_seq])) + 1\n",
    "num_classes = len(np.unique(Y_train))\n",
    "\n",
    "print(f'[INFO] Number of sentences in training set : {train_seq.shape[0]}')\n",
    "print(f'[INFO] Number of sentences in testing set  : {test_seq.shape[0]}')\n",
    "print(f'[INFO] Maximum sentence length in          : {train_seq.shape[1]}')\n",
    "print(f'[INFO] Vocabulary size                     : {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building basic CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"basic_cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30, 1)]           0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 30, 1, 128)        2211840   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 30, 32)            12320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 2,249,317\n",
      "Trainable params: 2,249,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[INFO] Saving model into ./models/basic_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "def make_cnn_model(input_shape, vocab_size):\n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    embs   = Embedding(vocab_size, 128, input_length=input_shape[1])(inputs)\n",
    "    embs   = Reshape(target_shape=(input_shape[1], 128))(embs)\n",
    "    \n",
    "    conv1d = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(embs)\n",
    "    pool1d = MaxPool1D(pool_size=2)(conv1d)\n",
    "    \n",
    "    # return_sequences : whether to return the full sequence of output or only the last output in the sequence\n",
    "    # return_sequences is defaulted as False.\n",
    "    rnn    = LSTM(64, return_sequences=False)(pool1d)\n",
    "    logits = Dense(num_classes)(rnn)\n",
    "    output = Activation(\"softmax\")(logits)\n",
    "    \n",
    "    model  = Model(inputs=inputs, outputs=output, name=BASIC_CNN_MODEL_NAME)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "train_seq = train_seq.reshape(-1, train_seq.shape[1], 1)\n",
    "test_seq  = test_seq.reshape(-1, test_seq.shape[1], 1)\n",
    "model = make_cnn_model(train_seq.shape, vocab_size)\n",
    "\n",
    "print(model.summary())\n",
    "print(f'[INFO] Saving model into {MODELS_DIR}/{BASIC_CNN_MODEL_NAME}.h5')\n",
    "model.save(f'{MODELS_DIR}/{BASIC_CNN_MODEL_NAME}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3252/3252 [==============================] - 87s 27ms/step - loss: 1.0276 - accuracy: 0.5919 - val_loss: 0.8970 - val_accuracy: 0.6359\n",
      "Epoch 2/10\n",
      " 964/3252 [=======>......................] - ETA: 58s - loss: 0.7919 - accuracy: 0.6861"
     ]
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_seq, Y_train, test_size=0.3333)\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10)\n",
    "\n",
    "print(f'[INFO] Saving model weights into {WEIGHTS_DIR}/{BASIC_CNN_MODEL_NAME}.weights.hdf5')\n",
    "model.save_weights(f'{WEIGHTS_DIR}/{BASIC_CNN_MODEL_NAME}.weights.hdf5')\n",
    "\n",
    "### Visualizing training results ###\n",
    "train_loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "\n",
    "train_acc = model.history.history['accuracy']\n",
    "val_acc = model.history.history['val_accuracy']\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(train_loss, label='Train Loss', color='blue')\n",
    "ax[0].plot(val_loss, label='Validation Loss', color='orange')\n",
    "\n",
    "ax[1].plot(train_acc, label='Train Accuracy', color='blue')\n",
    "ax[1].plot(val_acc, label='Validation Accuracy', color='orange')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
