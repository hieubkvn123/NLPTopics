{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "### Some Constants ###\n",
    "train_data_path = \"../data/sentiment_train.tsv\"\n",
    "test_data_path = \"../data/sentiment_test.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_table(train_data_path, header=0)\n",
    "test_df  = pd.read_table(test_data_path, header=0)\n",
    "features_text = train_df['Phrase'].values\n",
    "labels = train_df['Sentiment'].values\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2782/156060 [00:00<00:11, 13922.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleaning text data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156060/156060 [00:11<00:00, 13773.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of sentences : 121956\n",
      "[INFO] Number of empty sequences : 34104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Cleaning text data ...')\n",
    "\n",
    "def text_cleaning(text):\n",
    "    forbidden_words = set(stopwords.words('english'))\n",
    "    if text:\n",
    "        text = ' '.join(text.split('.'))\n",
    "        text = re.sub('\\/',' ',text)\n",
    "        text = re.sub(r'\\\\',' ',text)\n",
    "        text = re.sub(r'((http)\\S+)','',text)\n",
    "        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z]', ' ', text.strip().lower())).strip()\n",
    "        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n",
    "        text = [word for word in text.split() if word not in forbidden_words]\n",
    "        return text\n",
    "    return []\n",
    "\n",
    "features_text_cleaned = [] # array of text after cleaning\n",
    "labels_new = []\n",
    "num_empty = 0 # number of empty sequences after cleaning\n",
    "\n",
    "with tqdm.tqdm(total=len(features_text)) as pbar:\n",
    "    for i, x in enumerate(features_text):\n",
    "        text = text_cleaning(x)\n",
    "        if(len(text) <= 1): # do not accept sentences less that 2 words\n",
    "            num_empty += 1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        features_text_cleaned.append(' '.join(text))\n",
    "        labels_new.append(labels[i])\n",
    "        pbar.update(1)\n",
    "        \n",
    "features_text_cleaned = np.array(features_text_cleaned)\n",
    "labels = np.array(labels_new)\n",
    "print(f'[INFO] Number of sentences : {features_text_cleaned.shape[0]}')\n",
    "print(f'[INFO] Number of empty sequences : {num_empty}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['series escapades demonstrating adage good goose also good gander occasionally amuses none amounts much story'\n",
      " 'series escapades demonstrating adage good goose'\n",
      " 'escapades demonstrating adage good goose'\n",
      " 'escapades demonstrating adage good goose'\n",
      " 'demonstrating adage good goose' 'demonstrating adage' 'good goose'\n",
      " 'good goose' 'good goose' 'good goose'\n",
      " 'also good gander occasionally amuses none amounts much story'\n",
      " 'also good gander occasionally amuses none amounts much story'\n",
      " 'good gander occasionally amuses none amounts much story'\n",
      " 'gander occasionally amuses none amounts much story'\n",
      " 'gander occasionally amuses none amounts much story'\n",
      " 'occasionally amuses none amounts much story'\n",
      " 'occasionally amuses none amounts much story'\n",
      " 'amuses none amounts much story' 'none amounts much story'\n",
      " 'none amounts much story']\n"
     ]
    }
   ],
   "source": [
    "print(features_text_cleaned[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training size : 81308\n",
      "[INFO] Validate size : 40648\n",
      "(81308, 200) (81308,)\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 200\n",
    "tokenizer = Tokenizer(char_level=False, oov_token=\"<PAD>\")\n",
    "tokenizer.fit_on_texts(features_text_cleaned)\n",
    "\n",
    "features = tokenizer.texts_to_sequences(features_text_cleaned)\n",
    "features = pad_sequences(features, maxlen=max_seq_len)\n",
    "features = np.array(features)\n",
    "vocab_size = np.max(features) + 1\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(features, labels, test_size=0.3333)\n",
    "print(f'[INFO] Training size : {x_train.shape[0]}')\n",
    "print(f'[INFO] Validate size : {x_val.shape[0]}')\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 200, 128)          1914624   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 200, 128)          98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,112,901\n",
      "Trainable params: 2,112,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "inputs = Input(shape=(max_seq_len,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "model.add(inputs)\n",
    "model.add(Embedding(vocab_size, 128))\n",
    "# Add 2 bidirectional LSTMs\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "# Add a classifier\n",
    "model.add(Dense(5, activation=\"sigmoid\"))\n",
    "#model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1271/1271 [==============================] - 50s 39ms/step - loss: 1.0815 - accuracy: 0.5486 - val_loss: 0.9631 - val_accuracy: 0.5952\n",
      "Epoch 2/10\n",
      "1271/1271 [==============================] - 49s 39ms/step - loss: 0.8639 - accuracy: 0.6372 - val_loss: 0.9090 - val_accuracy: 0.6205\n",
      "Epoch 3/10\n",
      "1271/1271 [==============================] - 49s 39ms/step - loss: 0.7773 - accuracy: 0.6737 - val_loss: 0.9083 - val_accuracy: 0.6233\n",
      "Epoch 4/10\n",
      "1271/1271 [==============================] - 50s 39ms/step - loss: 0.7144 - accuracy: 0.6967 - val_loss: 0.9353 - val_accuracy: 0.6211\n",
      "Epoch 5/10\n",
      "1271/1271 [==============================] - 50s 39ms/step - loss: 0.6625 - accuracy: 0.7151 - val_loss: 0.9537 - val_accuracy: 0.6240\n",
      "Epoch 6/10\n",
      "1271/1271 [==============================] - 50s 39ms/step - loss: 0.6179 - accuracy: 0.7285 - val_loss: 1.0219 - val_accuracy: 0.6167\n",
      "Epoch 7/10\n",
      "1271/1271 [==============================] - 50s 39ms/step - loss: 0.5776 - accuracy: 0.7415 - val_loss: 1.0691 - val_accuracy: 0.6191\n",
      "Epoch 8/10\n",
      "1271/1271 [==============================] - 50s 39ms/step - loss: 0.5427 - accuracy: 0.7519 - val_loss: 1.1581 - val_accuracy: 0.6124\n",
      "Epoch 9/10\n",
      "1271/1271 [==============================] - 50s 39ms/step - loss: 0.5131 - accuracy: 0.7617 - val_loss: 1.2414 - val_accuracy: 0.6110\n",
      "Epoch 10/10\n",
      "1271/1271 [==============================] - 50s 39ms/step - loss: 0.4865 - accuracy: 0.7688 - val_loss: 1.2847 - val_accuracy: 0.6095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0b83f22370>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
