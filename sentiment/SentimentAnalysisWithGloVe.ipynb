{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysisWithGloVe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaql0JRl-C2J"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtUwtBk89Zo2",
        "outputId": "ac0c9af3-11a2-45a3-e2d2-5263d99147af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "### Tensorflow dependencies ###\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, Sequential \n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "### Scikit-learn dependencies ###\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "### nltk dependencies ###\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "### Download nltk data ###\n",
        "!python3 -m nltk.downloader punkt\n",
        "!python3 -m nltk.downloader stopwords\n",
        "!python3 -m nltk.downloader wordnet\n",
        "\n",
        "### Some constants ###\n",
        "true_path = \"/content/drive/My Drive/True.csv\"\n",
        "fake_path = \"/content/drive/My Drive/Fake.csv\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEWUTDfEb5W"
      },
      "source": [
        "# Loading and preprocessing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUPJALldFiXv"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "6Qc899J0_-Mf",
        "outputId": "1cd98f44-c43c-4ce1-f898-e1af5d9a2afc"
      },
      "source": [
        "df_true = pd.read_csv(true_path, header=0).sample(n=10000, random_state=np.random.randint(0,40000))\n",
        "df_fake = pd.read_csv(fake_path, header=0).sample(n=10000, random_state=np.random.randint(0,40000))\n",
        "\n",
        "df_true['label'] = 1\n",
        "df_fake['label'] = 0\n",
        "df = pd.concat([df_true, df_fake])\n",
        "\n",
        "df.head(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12150</th>\n",
              "      <td>Protesters injured in Honduras clashes as elec...</td>\n",
              "      <td>TEGUCIGALPA (Reuters) - Over two dozen people ...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>December 15, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15510</th>\n",
              "      <td>Casualties in religious attacks in Afghanistan...</td>\n",
              "      <td>KABUL (Reuters) - Civilian casualties in relig...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>November 7, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4400</th>\n",
              "      <td>U.S. attack on Syrian base killed five, Homs g...</td>\n",
              "      <td>BEIRUT (Reuters) - A U.S. missile strike on an...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>April 7, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4050</th>\n",
              "      <td>U.S. says strategy on North Korea centers on s...</td>\n",
              "      <td>WASHINGTON (Reuters) - The Trump administratio...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>April 26, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>951</th>\n",
              "      <td>White House chief of staff calls for special c...</td>\n",
              "      <td>WASHINGTON (Reuters) - White House Chief of St...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>October 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19706</th>\n",
              "      <td>China's highest-profile fugitive assailed by b...</td>\n",
              "      <td>BEIJING (Reuters) - China s highest profile fu...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>September 19, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8118</th>\n",
              "      <td>Mexico president following California marijuan...</td>\n",
              "      <td>(This Sept 15 story corrects “asked” in first...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>September 15, 2016</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19864</th>\n",
              "      <td>Venezuela's Maduro upbeat on talks, opposition...</td>\n",
              "      <td>CARACAS (Reuters) - President Nicolas Maduro h...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>September 16, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6046</th>\n",
              "      <td>Any UK-U.S. trade deal will put Britain first:...</td>\n",
              "      <td>LONDON (Reuters) - British Prime Minister Ther...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>January 25, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2635</th>\n",
              "      <td>McCain illness deprives Senate of crucial vote...</td>\n",
              "      <td>WASHINGTON (Reuters) - If John McCain’s illnes...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>July 20, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ... label\n",
              "12150  Protesters injured in Honduras clashes as elec...  ...     1\n",
              "15510  Casualties in religious attacks in Afghanistan...  ...     1\n",
              "4400   U.S. attack on Syrian base killed five, Homs g...  ...     1\n",
              "4050   U.S. says strategy on North Korea centers on s...  ...     1\n",
              "951    White House chief of staff calls for special c...  ...     1\n",
              "19706  China's highest-profile fugitive assailed by b...  ...     1\n",
              "8118   Mexico president following California marijuan...  ...     1\n",
              "19864  Venezuela's Maduro upbeat on talks, opposition...  ...     1\n",
              "6046   Any UK-U.S. trade deal will put Britain first:...  ...     1\n",
              "2635   McCain illness deprives Senate of crucial vote...  ...     1\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfTbp5DxFkzF"
      },
      "source": [
        "### Cleaning text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC2fE4ebEqgu"
      },
      "source": [
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii.decode()\n",
        "\n",
        "# Dictionary of English Contractions\n",
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "# Regular expression for finding contractions\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "# Function for expanding contractions\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # Assuming all nltk data is installed\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def filter_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = ' '.join(filtered)\n",
        "\n",
        "    ### Removing all special characters ###\n",
        "    special_chars = ['…', '–', '’', '‘', '”', '“']\n",
        "\n",
        "    for char_ in special_chars:\n",
        "      filtered_sentence = filtered_sentence.replace(char_, \"\")\n",
        "\n",
        "    # Removal of extra spaces\n",
        "    filtered_sentence = re.sub(' +',' ',filtered_sentence)\n",
        "    return filtered_sentence\n",
        "\n",
        "def preprocessing_text(text):\n",
        "  ### 1. Removing tags and accented data ###\n",
        "  text = cleanhtml(text)\n",
        "  text = remove_accents(text)\n",
        "\n",
        "  ### 2. Expanding contractions ###\n",
        "  text = expand_contractions(text)\n",
        "\n",
        "  ### 3. Removing punctuations + lowercasing ###\n",
        "  text = text.lower()\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "\n",
        "  ### 4. Removing special characters ### \n",
        "  #text = re.sub('[^A-Za-z0-9]+', ' ', text) # Remove non-text chars\n",
        "  #text = re.sub(' +',' ',text) # Remove extra spaces\n",
        "  #text = re.sub('\\w*\\d\\w*','', text) # Remove digits\n",
        "  \n",
        "  ### 5. Lemmatization and stopwords removal ###\n",
        "  text = filter_stopwords(text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMUeE_rrGw4e",
        "outputId": "6ab15ab0-109e-478d-ffed-80a9cd0ef41e"
      },
      "source": [
        "df['title'] = df['title'].apply(lambda x : preprocessing_text(x))\n",
        "df['text']  = df['text'].apply(lambda x : preprocessing_text(x))\n",
        "df['title'].head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12150    protester injured honduras clash electoral cri...\n",
              "15510    casualty religious attack afghanistan rise ste...\n",
              "4400     u attack syrian base killed five homs governor...\n",
              "4050     u say strategy north korea center sanction ope...\n",
              "951      white house chief staff call special counsel p...\n",
              "19706    china highestprofile fugitive assailed busines...\n",
              "8118     mexico president following california marijuan...\n",
              "19864    venezuela maduro upbeat talk opposition fear i...\n",
              "6046              ukus trade deal put britain first pm may\n",
              "2635     mccain illness deprives senate crucial vote tr...\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnKJZxnmJ8wk"
      },
      "source": [
        "# Feature engineering methods comparision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsFkJ8-nIuZj",
        "outputId": "23913af2-570f-4267-b0bf-4cda50e7d651"
      },
      "source": [
        "count_vec = CountVectorizer(min_df=5)\n",
        "bi_count_vec = CountVectorizer(min_df=3, ngram_range=(1,2))\n",
        "tf_idf_vec = TfidfVectorizer(min_df=5)\n",
        "\n",
        "features_text = df['text'].values\n",
        "targets = df['label'].values\n",
        "\n",
        "### Using Bag-of-words ###\n",
        "model = LogisticRegression()\n",
        "features = count_vec.fit_transform(features_text).toarray()\n",
        "x_train, x_val, y_train, y_val = train_test_split(features, targets, test_size=0.33333)\n",
        "model.fit(x_train, y_train)\n",
        "predictions = model.predict(x_val)\n",
        "accuracy_bow = accuracy_score(y_val, predictions)\n",
        "print(f'[INFO] Accuracy of logistic regression when using BOW : {accuracy_bow}')\n",
        "\n",
        "### Using N-gram Bow ###\n",
        "model = LogisticRegression()\n",
        "features = bi_count_vec.fit_transform(features_text).toarray()\n",
        "x_train, x_val, y_train, y_val = train_test_split(features, targets, test_size=0.33333)\n",
        "model.fit(x_train, y_train)\n",
        "predictions = model.predict(x_val)\n",
        "accuracy_ngram = accuracy_score(y_val, predictions)\n",
        "print(f'[INFO] Accuracy of logistic regression when using Ngram-BOW : {accuracy_ngram}')\n",
        "\n",
        "### Using Tf-Idf ###\n",
        "model = LogisticRegression()\n",
        "features = tf_idf_vec.fit_transform(features_text).toarray()\n",
        "x_train, x_val, y_train, y_val = train_test_split(features, targets, test_size=0.33333)\n",
        "model.fit(x_train, y_train)\n",
        "predictions = model.predict(x_val)\n",
        "accuracy_tf_idf = accuracy_score(y_val, predictions)\n",
        "print(f'[INFO] Accuracy of logistic regression when using BOW : {accuracy_tf_idf}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Accuracy of logistic regression when using BOW : 0.9946002699865006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owFCRjtK6tm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}